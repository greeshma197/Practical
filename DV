Practical 1:

Implement Data Loading, Storage and File Formats. Read data and store them in text format.

To understand the process of data loading, storage, and handling various file formats, and to implement these operations in a programmatic manner for reading and storing data in text format.

Objective:

•	Learn the significance of data loading, storage, and file formats in data processing.

•	Understand how to read data from files and store it efficiently.

•	Implement methods for handling data in text format (e.g., .txt, .csv, .json)

•	Ensure scalability, reliability, and accuracy in data handling operations.

Dataset used: 

We have used inbuilt python dictionary to create a dataset, and then with the help of pandas library we have save the file in various format.

Code:

import pandas as pd

 Step 1: Create Sample Data

data = {

    "Name": ["Alice", "Bob", "Charlie"],

    "Age": [25, 30, 35],

    "City": ["New York", "Los Angeles", "Chicago"]

}



 Create a DataFrame

df = pd.DataFrame(data)



 Step 2: Save Data in Text Format

text_file_path = "data.txt"



 Save as a CSV text file

df.to_csv(text_file_path, index=False, sep='\t')   Using tab-delimiter for text format

print(f"Data saved in text format at: {text_file_path}")



 Step 3: Read Data Back from the Text File

read_df = pd.read_csv(text_file_path, sep='\t')

print("Data read from the text file:")

print(read_df)



 Step 4: Save and Read in Other Formats

 Save as JSON

json_file_path = "data.json"

df.to_json(json_file_path, orient='records', lines=True)

print(f"Data saved in JSON format at: {json_file_path}")



 Read JSON

read_json_df = pd.read_json(json_file_path, orient='records', lines=True)

print("Data read from the JSON file:")

print(read_json_df)



 Save as Excel

excel_file_path = "data.xlsx"

df.to_excel(excel_file_path, index=False)

print(f"Data saved in Excel format at: {excel_file_path}")



 Read Excel

read_excel_df = pd.read_excel(excel_file_path)

print("Data read from the Excel file:")

print(read_excel_df)















import pandas as pd

import json

 Step 1: Create a More Complex Dataset

data = {

    "Name": ["Alice", "Bob", "Charlie", "David", "Eva"],

    "Age": [25, 30, 35, 40, None],

    "City": ["New York", "Los Angeles", "Chicago", "Houston", "Phoenix"],

    "Salary": [70000, 80000, 120000, 110000, None],

    "Department": ["HR", "Finance", "Engineering", "Management", "HR"]

}



 Create a DataFrame

df = pd.DataFrame(data)



 Step 2: Data Cleaning

 Handle missing values

df["Age"].fillna(df["Age"].mean(), inplace=True)

df["Salary"].fillna(df["Salary"].median(), inplace=True)



 Add a new calculated column

df["Tax"] = df["Salary"] * 0.2



 Step 3: Save Data in Multiple Formats

 Save as CSV

csv_file_path = "complex_data.csv"

df.to_csv(csv_file_path, index=False)

print(f"Data saved as CSV at: {csv_file_path}")



 Save as JSON

json_file_path = "complex_data.json"

df.to_json(json_file_path, orient='records', lines=True)

print(f"Data saved as JSON at: {json_file_path}")



 Step 4: Read Data Back and Perform Analysis

 Read from CSV

read_csv_df = pd.read_csv(csv_file_path)

print("Data read from CSV:")

print(read_csv_df)



 Read from JSON

read_json_df = pd.read_json(json_file_path, orient='records', lines=True)

print("Data read from JSON:")

print(read_json_df)



 Step 5: Advanced Analysis

 Group by Department and Calculate Mean Salary

grouped_df = df.groupby("Department")["Salary"].mean().reset_index()

print("Average Salary by Department:")

print(grouped_df)



 Save Grouped Data as Excel

excel_file_path = "grouped_data.xlsx"

grouped_df.to_excel(excel_file_path, index=False)

print(f"Grouped data saved as Excel at: {excel_file_path}")


Practical 2

Implement the code to interact with Web APIs and to perform web scrapping

The aim is to implement Python code that can interact with web APIs and perform web scraping. These functionalities enable automation for extracting structured and unstructured data from the web. This data can be used for applications such as data analysis, machine learning, and content aggregation.

Libraries Used:

•	BeautifulSoup (from bs4): For parsing HTML and XML documents.

•	requests: For sending HTTP requests and retrieving HTML content.

Dataset used: 

We have used the 'http://quotes.toscrape.com/’ website to scrape the data. Along that we have also used the OpenWeatherMap API to scape weather information.

Code:

import requests

from bs4 import BeautifulSoup



 Step 1: Send an HTTP request to the website

url = 'http://quotes.toscrape.com/'

response = requests.get(url)



 Check if the request was successful

    if response.status_code == 200:

     Step 2: Parse the HTML content of the page with BeautifulSoup

    soup = BeautifulSoup(response.text, 'html.parser')

    

     Step 3: Find all the quotes and authors

    quotes_data = []   To store the quotes and authors

    

    In the website, quotes are inside <span> tags with class 'text' and authors in <small> tags with class 'author'

    quotes = soup.find_all('span', class_='text')

    authors = soup.find_all('small', class_='author')

Step 4: Loop through the quotes and authors, and store them in a dictionary

    for i in range(len(quotes)):

        quote_text = quotes[i].text

        author = authors[i].text

        quotes_data.append({'quote': quote_text, 'author': author})

    

Step 5: Display the scraped data

    for entry in quotes_data:

        print(f"Quote: {entry['quote']}")

        print(f"Author: {entry['author']}")

        print('---')

else:

    print(f"Failed to retrieve the webpage. Status code: {response.status_code}")



Scraping using API Endpoint

import requests

 Replace with your actual OpenWeatherMap API key

API_KEY = '2394ceb479e01f28ff48046577fc4e10'   Ensure this is correct

city = 'London'   Adding country code to avoid ambiguity

 Define the API endpoint and include your API key

url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={API_KEY}&units=metric"

 Send a GET request to the API

response = requests.get(url)

 Check if the request was successful

if response.status_code == 200:

    data = response.json()

    print(f"City: {data['name']}")

    print(f"Temperature: {data['main']['temp']}°C")

    print(f"Weather: {data['weather'][0]['description']}")

else:

    print(f"Failed to retrieve data. Status code: {response.status_code}, Reason: {response.reason}")


Practical 3

Demonstrate Data Cleaning and Preparation.

To clean and prepare raw data for analysis by identifying and handling missing, inconsistent, and irrelevant data, ensuring it is accurate and complete for generating meaningful insights.

•	Identify data quality issues: Detect missing, inconsistent, or duplicate entries in the dataset.

o	Missing values can occur due to human error, system issues, or incomplete data collection.

o	Missing values are identified as blanks, NaNs, or nulls.

•	Handle missing data: Apply appropriate techniques to address missing values (e.g., imputation, removal).

o	Imputation: Filling missing values using statistical methods like mean, median, or mode.

o	Removal: Eliminating rows or columns with excessive missing values.

Dataset used: 

For the study purpose we have use the Titanic dataset which is very known dataset in the data science community. This dataset is related to voyage from Southampton England to New York, in a way of transits it met mishap and it’s was recorded has one of the awful events. 



Code:

import pandas as pd

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler, LabelEncoder



Step 1: Load the Titanic Dataset

titanic = sns.load_dataset('titanic')



Step 2: Inspect the Dataset

print("First 5 rows of the Titanic dataset:")

print(titanic.head())



Step3: Check for missing values

print("\nMissing values in each column:")

print(titanic.isnull().sum())

Check data types

print("\nData types and basic statistics:")

print(titanic.info())



Check stastical summary

print("\n statistical Summary:")

print(titanic.describe())



Check shape of dataset

print("\n Shape of dataset:")

print(titanic.shape)



Step 3: Handle Missing Values

Fill missing 'age' values with the median of the 'age' column

titanic['age'].fillna(titanic['age'].median(), inplace=True)



Fill missing 'embarked' values with the most common value (mode)

titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)



Drop 'deck' column due to a large number of missing values

titanic.drop(columns=['deck'], inplace=True)



Fill missing 'embark_town' with the mode

titanic['embark_town'].fillna(titanic['embark_town'].mode()[0], inplace=True)



Drop any remaining rows with missing values

titanic.dropna(inplace=True)

print("\nMissing values after cleaning:")

print(titanic.isnull().sum())


Practical 4

Implement Data wrangling on a data set. 



The aim of data wrangling is to clean, structure, and enrich raw data into a usable format for analysis. This process ensures that the data is accurate, consistent, and suitable for generating insights or building machine learning models.



Data wrangling involves several systematic steps to prepare raw data for further analysis. 



These steps include:

•	Data Acquisition

•	Exploratory Data Analysis (EDA)

•	Data Cleaning

•	Data Transformation

•	Data Integration

•	Feature Engineering

•	Data Validation

•	Storage and Export



A cleaned and structured dataset free from missing values, duplicates, and inconsistencies.

Enhanced data usability for statistical analysis and predictive modelling.

Increased confidence in the reliability and quality of the results derived from the data.



Code:

 Step 1: Handle Categorical Variables

 Convert 'sex' and 'embarked' into numerical labels using LabelEncoder

label_encoder = LabelEncoder()

titanic['sex'] = label_encoder.fit_transform(titanic['sex'])

titanic['embarked'] = label_encoder.fit_transform(titanic['embarked'])





 Convert 'who' into binary (man: 1, woman: 0)

titanic['who'] = titanic['who'].apply(lambda x: 1 if x == 'man' else 0)





 Step 2 : Feature Engineering

 Create a new feature: 'family_size' = 'sibsp' + 'parch' + 1

titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1

 Step 3 : Remove Duplicates

titanic_cleaned = titanic.drop_duplicates()



 Step 4 : Handle Outliers

 Inspecting outliers in the 'fare' column

plt.figure(figsize=(8,4))

plt.boxplot(titanic_cleaned['fare'])

plt.title("Box plot for Fare (outliers detection)")

plt.show()



 Cap outliers in 'fare' to the 99th percentile

fare_cap = titanic_cleaned['fare'].quantile(0.99)

titanic_cleaned['fare'] = np.where(titanic_cleaned['fare'] > fare_cap, fare_cap, titanic_cleaned['fare'])



print("\nFare column statistics after handling outliers:")

print(titanic_cleaned['fare'].describe())



 Step 5: Normalize Numerical Data

 Normalize 'age' and 'fare' using StandardScaler

scaler = StandardScaler()

titanic_cleaned[['age', 'fare']] = scaler.fit_transform(titanic_cleaned[['age', 'fare']])



print("\nFirst 5 rows of the cleaned and wrangled data:")

print(titanic_cleaned.head())



 Step 6: Save the cleaned data to a CSV file

titanic_cleaned.to_csv('titanic_cleaned.csv', index=False)

print("\nCleaned dataset saved to 'titanic_cleaned.csv'")


Practical 5



Demonstrate the handling of missing data and string manipulation.



To demonstrate techniques for handling missing data and string manipulation in datasets to ensure data quality and consistency.



Objectives:



•	Understand the importance of handling missing data and its impact on analysis.

•	Learn various methods to identify and address missing values.

•	Explore string manipulation techniques to clean and preprocess text data.

•	Enhance data quality for accurate and reliable insights.



Dataset used: For the demonstration purpose on how to handle missing data, along with string manipulation we have created a toy dataset frame using pandas. Next we have also imported the penguin data that comes preinstalled with Seaborn library.



Code:

import pandas as pd

Step 1: Create a Toy Dataset

data = {

    "Product": ["Laptop", "Tablet", "Smartphone", "Monitor", None],

    "Price": [1200, None, 800, 300, 150],

    "Category": ["Electronics", "Electronics", None, "Accessories", "Accessories"],

    "Description": ["High-end laptop", "Compact and versatile", "Feature-packed smartphone", None, "Affordable monitor"]

}

Create a DataFrame

df = pd.DataFrame(data)

print("Original Dataset:")

print(df)



Step 2: Handling Missing Data

Fill missing Product names with "Unknown"

df["Product"].fillna("Unknown", inplace=True)



Fill missing Prices with the median price

df["Price"].fillna(df["Price"].median(), inplace=True)



Fill missing Categories with "Miscellaneous"

df["Category"].fillna("Miscellaneous", inplace=True)



Fill missing Descriptions with a default value

df["Description"].fillna("No description available", inplace=True)

print("\nDataset After Handling Missing Data:")

print(df)



Step 3: String Manipulation

Add a new column "Short Description" with the first 10 characters of the Description

df["Short Description"] = df["Description"].str[:10]



Convert the Category column to uppercase

df["Category"] = df["Category"].str.upper()





Replace spaces with hyphens in the Product column

df["Product"] = df["Product"].str.replace(" ", "-", regex=False)

print("\nDataset After String Manipulation:")

print(df)





Step 4: Save the Cleaned and Transformed Dataset

cleaned_file_path = "cleaned_data.csv"

df.to_csv(cleaned_file_path, index=False)

print(f"\nCleaned data saved as CSV at: {cleaned_file_path)

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt



 Step 1: Load the Dataset

 Load the Penguins dataset from seaborn

penguins = sns.load_dataset("penguins")



 Display the first few rows of the dataset

print("Original Dataset:")

print(penguins.head())



 Step 2: Exploratory Data Analysis (EDA)

 Check dataset info

print("\nDataset Info:")

print(penguins.info())



 Describe numerical columns

print("\nStatistical Summary of Numerical Columns:")

print(penguins.describe())





 Check for missing values

print("\nMissing Data:")

print(penguins.isnull().sum())





 Visualizations

 Distribution of bill length

plt.figure(figsize=(8, 6))

sns.histplot(penguins['bill_length_mm'], kde=True, bins=20, color="blue")

plt.title("Distribution of Bill Length")

plt.xlabel("Bill Length (mm)")

plt.ylabel("Frequency")

plt.show()



Boxplot for flipper length by species

plt.figure(figsize=(8, 6))

sns.boxplot(x="species", y="flipper_length_mm", data=penguins, palette="Set2")

plt.title("Flipper Length by Species")

plt.xlabel("Species")

plt.ylabel("Flipper Length (mm)")

plt.show()



 Correlation heatmap

plt.figure(figsize=(10, 8))

sns.heatmap(penguins.select_dtypes('float64').corr(), annot=True, cmap="coolwarm", fmt=".2f")

plt.title("Correlation Heatmap")

plt.show()



Step 3: Handle Missing Data

Fill missing numerical values with their respective column means

penguins["bill_length_mm"].fillna(penguins["bill_length_mm"].mean(), inplace=True)

penguins["bill_depth_mm"].fillna(penguins["bill_depth_mm"].mean(), inplace=True)

penguins["flipper_length_mm"].fillna(penguins["flipper_length_mm"].mean(), inplace=True)

penguins["body_mass_g"].fillna(penguins["body_mass_g"].median(), inplace=True)



Fill missing categorical values with "Unknown"

penguins["sex"].fillna("Unknown", inplace=True)

print("\nDataset After Handling Missing Data:")

print(penguins.isnull().sum())



 Step 4: String Manipulation

 Create a new column by concatenating species and island

penguins["species_island"] = penguins["species"] + " - " + penguins["island"]



 Convert species names to uppercase

penguins["species"] = penguins["species"].str.upper()



 Extract the first three letters of the island name as a new column

penguins["island_code"] = penguins["island"].str[:3]

print("\nDataset After String Manipulation:")

print(penguins.head())



 Step 5: Save the Cleaned and Transformed Dataset

cleaned_file_path = "cleaned_penguins.csv"

penguins.to_csv(cleaned_file_path, index=False)

print(f"\nCleaned dataset saved as CSV at: {cleaned_file_path}")



 Step 6: Grouped Analysis

 Group by species and calculate mean flipper length

grouped_data = penguins.groupby("species")["flipper_length_mm"].mean().reset_index()

print("\nMean Flipper Length by Species:")

print(grouped_data)



 Save grouped data as Excel

grouped_file_path = "grouped_penguins.xlsx"

grouped_data.to_excel(grouped_file_path, index=False)

print(f"\nGrouped data saved as Excel at: {grouped_file_path}"


Practical 7



Perform sorting and filtering using tableau, create visualizations and publish it on Tableau Cloud. 



To utilize Tableau for sorting and filtering data, creating insightful visualizations, and publishing them on Tableau Cloud for collaboration and sharing.



Step 1: Prepare the Dataset



1.	Load your dataset in a structured format (e.g., Excel, CSV, or a database).

2.	Ensure the data is clean and properly formatted with no missing values or inconsistent types.

3.	Save the dataset to your local system.



Step 2: Open Tableau Desktop



1.	Launch Tableau Desktop on your computer.

2.	Click on Connect to Data and choose your data source (Excel, CSV, SQL, etc.).

3.	Load the dataset by selecting the file or connecting to the database.

4.	Drag and drop the relevant tables into the Canvas if needed.



Step 3: Perform Sorting



1.	Create a Sheet:

o	Navigate to the Sheets tab in Tableau.

o	Drag a measure (e.g., Sales, Profit) to the Rows shelf and a dimension (e.g., Region, Category) to the Columns shelf.

2.	Apply Sorting:

o	Hover over the Axis or Header of the visualization (e.g., Category or Region).

o	Click the Sort button (ascending or descending order) visible on the axis.

o	Alternatively, right-click on the field in the Columns or Rows shelf and select Sort. Choose:

	Sort Order: Ascending or Descending.

	Sort By: Field (e.g., Profit) or Manual (drag items to reorder manually).



Step 4: Apply Filtering



1.	Drag the desired dimension (e.g., Region, Product) to the Filters shelf.

2.	A dialog box will appear. Choose the values you want to include or exclude and click OK.

3.	To add an interactive filter:

o	Right-click on the field in the Filters shelf.

o	Select Show Filter to display a filter control on the dashboard or worksheet.

4.	Customize the filter display by choosing options like:

o	Single Value (Dropdown).

o	Multi-Value (Checkbox).

o	Range of Dates or Numbers.

Step 5: Create Visualizations



1.	Use the Marks Card to change the visualization type (Bar, Line, Pie, etc.).

2.	Drag fields to the Columns, Rows, and Marks shelves to build your visualizations. Examples:

o	Bar Chart: Drag a measure (e.g., Sales) to Rows and a dimension (e.g., Category) to Columns.

o	Pie Chart: Select a dimension (e.g., Region) and measure (e.g., Profit) and choose the Pie Chart from the Marks dropdown.

o	Map: Use geographical fields like Country, State, or City to create maps.

3.	Add Colors, Labels, and Tooltips from the Marks card to enhance the visualizations.



Step 6: Build a Dashboard



1.	Click on the Dashboard tab.

2.	Drag your sheets onto the dashboard canvas.

3.	Customize the layout by resizing and arranging the sheets.

4.	Add interactivity using filter actions:

o	Click Dashboard > Actions > Add Action > Filter.



Step 7: Publish to Tableau Cloud



1.	Ensure you have a Tableau Cloud account. If not, create one at Tableau Cloud.

2.	Go to Server > Sign In in Tableau Desktop and log in with your Tableau Cloud credentials.

3.	After signing in:

o	Click File > Publish to Tableau Cloud.

o	Choose the project folder where you want to save the workbook.

o	Provide a descriptive name for your workbook.

4.	Adjust Permissions and Data Source Settings as needed.

5.	Click Publish to upload the workbook to Tableau Cloud.



Step 8: Share and View Online



1.	Log in to your Tableau Cloud account via a web browser.

2.	Navigate to the published workbook.

3.	Use the share link or invite others by clicking Share and sending invitations.

4.	Users can view and interact with your visualizations online

