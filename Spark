Practical 03

Aim: Filtering RDDs, and the Minimum Temperature by Location Example

# Initialize Spark
from pyspark import SparkConf, SparkContext

conf = SparkConf().setMaster("local").setAppName("MinTemperatures")
sc = SparkContext(conf = conf)

# Function to parse line
def parseLine(line):
    fields = line.split(',')
    stationID = fields[0]
    entryType = fields[2]
    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0
    return (stationID, entryType, temperature)

# Find Minimum Temperature City
lines = sc.textFile("1800.csv")
parsedLines = lines.map(parseLine)
minTemps = parsedLines.filter(lambda x: "TMIN" in x[1])
stationTemps = minTemps.map(lambda x: (x[0], x[2]))
minTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))
results = minTemps.collect()

# Show the result for TMIN Entry Type
for result in results:
    print(result[0] + "\t{:.2f}C".format(result[1]))

# Find Maximum Temperature City
maxTemps = parsedLines.filter(lambda x: "TMAX" in x[1])
stationTemps = maxTemps.map(lambda x: (x[0], x[2]))
maxTemps = stationTemps.reduceByKey(lambda x, y: max(x,y))
results = maxTemps.collect()

# Show the result for TMAX Entry Type
for result in results:
    print(result[0] + "\t{:.2f}C".format(result[1]))

Practical 04

Aim: Counting Word Occurrences using flatmap()

from pyspark import SparkConf, SparkContext

conf = SparkConf().setMaster("local").setAppName("WordCount")
sc = SparkContext(conf = conf)

input = sc.textFile("file:///sparkcourse/book.txt")
words = input.flatMap(lambda x: x.split())
wordCounts = words.countByValue()

for word, count in wordCounts.items():
    cleanWord = word.encode('ascii', 'ignore')
    
    if cleanWord:
        print(cleanWord.decode() + " " + str(count))

Practical 05

Aim: Executing SQL Commands and SQL-Style Functions on a DataFrame in Spark


from pyspark.sql import SparkSession
from pyspark.sql import Row
import collections

# Create a SparkSession (Note, the config section is only for Windows!)
spark = SparkSession.builder.config("spark.sql.warehouse.dir", "file:///C:/temp").appName("SparkSQL").getOrCreate()

def mapper(line):
    fields = line.split(',')
    return Row(ID=int(fields[0]), name=str(fields[1].encode("utf-8")), age=int(fields[2]), numFriends=int(fields[3]))

lines = spark.sparkContext.textFile("fakefriends.csv")
people = lines.map(mapper)

# Infer the schema, and register the DataFrame as a table.
schemaPeople = spark.createDataFrame(people).cache()
schemaPeople.createOrReplaceTempView("people")

# SQL can be run over DataFrames that have been registered as a table.
teenagers = spark.sql("SELECT * FROM people WHERE age >= 13 AND age <= 19")

# The results of SQL queries are RDDs and support all the normal RDD operations.
for teen in teenagers.collect():
    print(teen)

# We can also use functions instead of SQL queries:
schemaPeople.groupBy("age").count().orderBy("age").show()

spark.stop()

Practical 06

Aim: Implement Total Spent by Customer with DataFrames

from pyspark import SparkConf, SparkContext

conf = SparkConf().setMaster("local").setAppName("SpendByCustomer")
sc = SparkContext(conf = conf)

def extractCustomerPricePairs(line):
    fields = line.split(',')
    return (int(fields[0]), float(fields[2]))

input = sc.textFile("file:///sparkcourse/customer-orders.csv")
mappedInput = input.map(extractCustomerPricePairs)
totalByCustomer = mappedInput.reduceByKey(lambda x, y: x + y)
results = totalByCustomer.collect();

for result in results:
    print(result)

Practical 07

Aim: Use Broadcast Variables to Display Movie Names Instead of ID Numbers


from pyspark.sql import SparkSession, Row, functions

def loadMovies():
    movieNames = {}
    with open("c:\\Users\\gagan\\Downloads\\ml-100k\\u.item") as f:
        for line in f:
            fields = line.split('|')
            movieNames[int(fields[0])] = fields[1]
    return movieNames

spark = SparkSession.builder.appName("Broadcast Variable").getOrCreate()

nameID = loadMovies()
nameID_broadcast = spark.sparkContext.broadcast(nameID)

lines = spark.sparkContext.textFile("u.data")

movies = lines.map(lambda x: Row(movieID= int(x.split()[1])))

movieDataset = spark.createDataFrame(movies)

topMoviesID = movieDataset.groupBy("movieID").count().orderBy("count",
                                                              ascending=False).cache()

def addMovieName(movieID):
    return nameID_broadcast.value[movieID]

addMovieNameUDF = functions.udf(addMovieName)

topMoviesWithNames = topMoviesID.withColumn("movieName",
                                            addMovieNameUDF(functions.col("movieID")))

topMoviesWithNames.select("movieName", "count").show(10, truncate=False)

spark.stop()

Practical 08

Aim: Using Spark ML to Produce Movie Recommendations


from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('recommendation').getOrCreate()

from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.recommendation import ALS

data = spark.read.csv('ratings.csv', inferSchema=True, header=True)

data.head()

data.printSchema()

data.describe().show()

train, test = data.randomSplit([0.8, 0.2], seed=42)

als = ALS(maxIter=5, regParam=0.01, userCol="userId", itemCol="movieId", ratingCol="rating")

model = als.fit(train)

# Evaluate the model by computing the RMSE on the test data
predictions = model.transform(test)

predictions.show()

evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating", predictionCol="prediction")

rmse = evaluator.evaluate(predictions)

print("Root-mean-square error = " + str(rmse))

single_user = test.filter(test['userId'] == 3).select(['movieId', 'userId'])

single_user.show()

recommendations = model.transform(single_user)

recommendations.orderBy('prediction', ascending=False).show()

Practical 09

Aim: Use Windows with Structured Streaming


from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Create a local StreamingContext with two working thread and batch interval of 1 second
sc = SparkContext("local[2]", "NetworkWordCount")
ssc = StreamingContext(sc, 1)

# Create a DStream that will connect to localhost:9999
lines = ssc.socketTextStream("localhost", 9999)

# Split each line into words
words = lines.flatMap(lambda line: line.split(" "))

# Count each word in each batch
pairs = words.map(lambda word: (word, 1))
wordCounts = pairs.reduceByKey(lambda x, y: x + y)

# Print the first ten elements of each RDD generated in this DStream to the console
wordCounts.pprint()

ssc.start()  # Start the computation
ssc.awaitTermination()  # Wait for the computation to terminate

# May cause deprecation warnings, safe to ignore, they aren't errors
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.sql import SQLContext
from pyspark.sql.functions import desc

sc.stop()
ssc.stop()

sc = SparkContext()
socket_stream = ssc.socketTextStream("127.0.0.1", 5555)

# ... rest of the code ...
